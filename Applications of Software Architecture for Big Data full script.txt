0:10
Hi all, welcome to Applications of Software Architecture for Big Data. My name is Mike Baroneck and I'm with Tyson Gurn next to me. >> Hey everyone. >> Hey, and we're going to be your instructors for the course. Maybe something to mention. This course is the third in a three part series, so the first was Fundamentals of Software Architecture for Big Data. The next patterns or Architecture Patterns for Big Data. And this you could think of as the capstone or the project for the earlier two courses, Applications of Software Architecture for Big Data. So maybe a couple of things to talk about, a little bit of the project history, maybe some of our goals, and then maybe a little bit of a preview in terms of what you're going to do as part of this course. So I guess a couple of things. Why the architecture that we selected or that we're going to put forward for you? It's something that we've used over the years. I would say, jeez, I've been writing software a little while, so I probably used this style of architecture first in maybe the early 2000s. And so what it includes is something that's going to go out and collect data. You're going to have something that analyzes data and then a web application that then displays that data. And I guess the practical side of this, we're going to build, we're going to deploy, we're going to run it, we're going to treat this like a regular project that we might do out and about in the industry when folks are building software. So we're trying to share that bit of day in the life with you and maybe some of the lessons learned along the way that we've experienced in the positive sense. So that's a little bit of the history. Maybe do you want to talk about maybe a little bit more specifics and how it relates to kind of the modern-day architecture? >> Sure, yeah. So this architecture was really popularized by Heroku, which is a deployment platform that was really one of the first deployment platforms that made it easy to get apps to prod without needing to worry about how they are deployed. So GitPush, Heroku Main, if you've ever worked with Heroku, will kind of magically get your app up and running without you having to provision anything or worry about the infrastructure. And Heroku kind of had this idea of a proc file where you could define your stateless web app that was running, and then maybe a couple of workers that would be running in the background. And as Mike mentioned, these workers might be collecting data, analyzing data, they might be running on a cron, they might be responding to other events in your system. But this architecture Heroku made really easy to get up and running in production and it's really taken off since then. >> Yeah, and I think another thing around that time frame, let's say it was maybe 2010, 2012 or s, it was Ruby on rails. And Rails was this really lightweight framework created by an individual David Hannameyer Hansen. And that with Heroku just opened up the world for folks to come into the world of software and building software. And yeah, as Tyson said, there was a proc file that kind of embodies the architecture that we're going to go after as part of our project here. So switching gears slightly, there's a couple of other things I talked about the day in the life and us sharing some of our experiences, and some of these were talked about in the Fundamentals of Software Architecture for Big Data. But it's things like, let's write some tests so we have confidence when we deploy applications. Let's make sure we bake in metrics and monitoring early so that makes it easier for us to run those applications we built. Naturally, there's the automation of that. You might have heard continuous delivery or continuous integration, terms like that we'll talk about later in the course as we set up the different exercises for you. And then I guess at a high level there's a few things that are fairly common. One is just simply interacting with the database. And Heroku had that out of the box as well as maybe talking to an external API. So you could think of something as simple as collecting data from Twitter and then doing something with that data is the analysis piece. There's also when you get into kind of distributed systems where they become more workers and maybe there's more shared responsibilities, you might interact with the different services via message queue. And that brings you a little bit forward to what cloud native and some of these cloud-native architectures you might have heard about, it's really getting at almost like single-responsibility apps or single-responsibility services. And in our case, we're going to do that, one is going to collect data, one is going to analyze data, and then our web app is going to display that information. So I think we're off to the races in terms of the project. Anything else other than the setup or that we should say as part of the setup. >> I think we're ready to go. So we'll dive into things like the Rubric, we'll talk about maybe the whiteboard architecture that we're going after, and then we have a little example code base that will show you what we've done to get closer to the architecture that we described. So good luck. Should be fairly fun. And I guess one last thing. We've interviewed a ton of candidates over the last decade or two. And having this type of experience where you put together a project and that project really embodies what's current and what folks are building at some of the different companies, whether it's a startup or whether it's an enterprise, it goes a real long way. And maybe one last thing, the examples we're going to use, we also run a workshop as part of the company we're at called Initial Capacity. So you're going to see some of our Git repos and some of the Initial Capacity project starters that we'll share with you to get your project bootstrapped. That's it for now. We'll see you on the next video, thanks.

0:04
We're back, and we're going to talk about the example Codebase that we mentioned. If you navigate to github.com/initialcapacity, you'll see a project or a repo called the kotlin-ktor-starter. If you click on that repo, the first thing you'll notice is a read me, and that read me describes a little bit of the intent in terms of what we're going after for the project. I would say the first thing that's here is this application continuum link, and I'd encourage you to take a look at it if you hadn't already done so as part of one of our earlier courses. This also talks about the architecture that we're going after. It goes a little bit deeper and actually takes you through the steps needed to evolve an architecture, like the one we're getting at. I would maybe spend a little bit of time taking a look at this app continuum article. There's also a Codebase associated with this article. But if we bounce back to the Kotlin Ktor starter, I'm sure there's the basic setup which has our web application or data collector or analyzer. Then also what we talked about here's the tech stack. As part of one of your assignments, we'll probably dive a little bit deeper into the tech stack, and then ask individuals to talk about or write up the tech stack that they're going after. That'll be in association with the whiteboard architecture that we'll ask you to do as well. But here, the tech stack that we're using in this example is Kotlin Web Framework Ktor. We're using free marker to display or to take some code and turn it into HTML is basically what free markers doing for us. We have a build tool we're going to test with JUnit, and then we're going to deploy it at Google Cloud. But this tech stack is fairly typical and whether you're writing a Ruby application, a Python application, you have the same pieces that make up the tech stack. The Python example maybe is something like Pythons language and the web framework that you'll be using is something like Flask. It turns out that a lot of the documentation in the course itself is going to walk you through doing similar with Python. The example that we're using here is Kotlin Ktor, maybe a little bit intentional, something that folks wouldn't necessarily pick up. Although you're more than welcome to use this as a starting point as well for your project. But our hunches, most people might gravitate over to Python. So a lot of our examples actually include Python. Maybe one other thing here while we're here, if I bounce over to the Codebase for a minute, you'll see the top-level project. So pick your IDE of choice or not. You could pick VI or a text editor too, if you'd like. But we're big fans of code editors and we're using IntelliJ for the example. There's also Visual Studio Code, which I think is fairly common for folks. But you should have something along the lines like what I'm showing here, so you have few different applications. The ones we've talked about, the web app analyzer collector, and then you might have some components that support those different applications. For us, if you looked at the app continuum article, it'll talk about more of the separation between applications and components. At a high level think, this in the applications area is where we configure and we talk about maybe bootstrapping, write the application, and then the components areas where there's more of the, I guess, the meat of the application, whether that's collecting data or reaching out to a public API to do some machine learning and to analyze data. Then naturally, we could run tests here, both fast tests that some might break out into unit acceptance and integration tests. That's the beginning project and this will give folks a look at a working example, or at least the start of a project, I should say. Again, feel free to use any language you want. Whether that's Ruby Rails deployed to Heroku, like we talked about in the intro, or Python, which is captured in a lot of documentation around the course, or even Kotlin and Ktor, which is our example here. I think that's everything, and we'll see you on the next video.

this is the mentioned codebase kotlin-ktor-starter : https://github.com/initialcapacity/kotlin-ktor-starter 
please check this out. it's must.
0:04
We're back, and we wanted to talk a little bit about project selection. If you remember from the last screencast, we've talked about our Kotlin Ktor starter. I guess I would say the one thing about this application, the focus was maybe the tech stack, the technology, how we organize things. We talked about reading this app continuum article. That's a little bit different than the project selection. For the project selection, I guess there's a couple of things to think about. One, it may just simply be something that you woke up one day and said, oh, my gosh, wouldn't it be cool to build that? Something you enjoy. Maybe context for another class that you're taking or maybe something you're doing day-to-day at work or maybe one of your hobbies. Well, here I'll give you an example. One thing I built a while back, I was interested in finding a used 1987 Toyota Land Cruiser. I wrote some software that would go out to the different Toyota dealers in the country actually. Then I would look for that model. I would actually be able to select color, and I would display it on a webpage. I even had an alert that would come up and say, we found the Land Cruiser and it's in Boise, Idaho or something like that. But the interesting thing is it embodied the architecture that we're talking about as part of this course. It had something that went out and collected data, scraping, basically RSS information from Toyota dealerships. I would then analyze the data by looking for certain make, model, and then color even in this example. Then also what? I would then display it on a webpage. In the morning I could refresh a webpage and see if there were any 1987 Land Cruisers out and about in the country today. That's maybe one example of a project that I did. Naturally, I've used this type of architecture for a long time. It worked out nicely. Go collect data, do some analysis, and then view that data as a webpage. I'm going to bounce to a webpage for a second. This is the course that we use. This is the first course in the series. There's an application called Provenance here that we could share the codebase for. While this one doesn't embody the actual or the exact architecture, at a high level, it has the same components. It's collecting data from Info Q and the example here, and then it's analyzing the data and then displaying the data. Provenance was really getting at the information that we're reading on the webpage real or is it not? The idea was to go after the author's writing the articles as opposed to where those articles are being published or even republished. It was really nice way to say, I have 10 people that I enjoy reading their articles. I trust those people. I want a small browser plugin that gives me all their recent articles. This was the Provenance application. Again, we use this in the first of the courses in the three-part series. Tyson had one as well that he was going to describe, so let him describe that one. I have a similar app. I had a couple Instagram feeds that I wanted to follow, but I didn't want to get a Instagram account, install Instagram on my phone, that thing. I thought, well, it'd be nice to consume those with RSS. So I built an app that goes out and scrapes Instagram, like reads their private GraphQL API and stores that data in Redis. Interestingly enough that worker actually has to run on a Raspberry Pi in my basement because Instagram has blocked all the Cloud providers from making those requests. Then there's another background worker that takes that unstructured data from Redis and puts it in a database and then the web app serves up the RSS feeds, so I can read on my favorite RSS reader. Just to be clear, there's not a Raspberry Pi requirement for the project at all. Sometimes you need some grounds for where to deploy things. Project selection, figure out something right to use in terms of the context. Something you enjoy. Again, somewhere you could collect data, somewhere you could go and analyze data or somehow to analyze data. Maybe you use a public API to help you with that analysis. Then naturally the web application to display the data that you collected and analyzed. That's three examples of the project or three examples of a project that individuals could use. Anything else in terms of project selection? I think we're good to go. We'll see you soon on tech stack and whiteboard architecture. Thanks again.

To get your project started, you should define the product that you are going to deploy. Some specific points you should be concerened with include:

What problem is the product aimed at solving?

Who is the product geared towards (targeted audience)?

How is my product unique?

Answering the above questions will give you a sense of the features that the product should entail. It is important to keep in mind that the features should align with the answers to the aforementioned questions.  For example, if I were to develop a website that tackles booking hotel rooms for holiday travelers, then adding a feature which allows users to buy a car would not be relevant. 

Once you have figured out the functuionality you would like your product to take on, you can formally begin to document the requirements for the product. Typically, requirements can be broken down into two categories, namely, user requirements and system requirements. User requirements specify the funtionality interms of what the person using the product should experience. Whereas, system requirements define the operations and constraint of the system. An example of the a user requirement would be that a customer for the booking website should be able to create an account if one does not already exist. A system requirements would specify that a NoSQL database should be used to store user information.

In order to start implements features that conform to the requirements, we write user stories. User stories are covered in the next section.

User stories are narratives that describe how a user interacts with the system for a specific scenario. User stories are distinct from implementation tasks. You want each user story to be a short description of functionality that would be valuable to one of the users of your application. You will decompose a user story into implementation tasks when you actually start to work on a user story. Stories document functionality your system should provide to a user and tasks document a development-related work item that needs to be completed to make progress on a story. As such, it should be clear in your user story what feature is being described and for whom (if your system has more than one type of user) and it should be clear for each task which story it is helping to complete (
Gherkin
-esque format). All user stories should be annotated with points.

An example of a user story is:

As a user, I want to sign in so I can see my marketing campaigns

0:03
Let's talk about the architecture for your project and we have a whiteboard here that I'm going to sketch the architecture that we're going after. Okay, so the first thing you'll note from the rubric is you're going to have one component that we're going to call the web application. We're also going to have another component that collects data or the data collector and we're going to have another component that analyzes data or the data analyzer. Okay, so the first thing maybe to call out is that each of these need to be free running processes. So, when you're building your project or the applications for your project, you really need to think there's three applications, and if you were to run them locally on your machine, you would see each of the three processes running. Okay, so we're building three apps, or we could use application as the generic term. Okay, so first off, the data collector is going to go out and collect data. And we could use a handful of different things, but typically we'd reach out to a public API, maybe like Twitter or maybe we're going after some weather data, that's public dataset. And so we're going to collect that data and that data is going to be dropped off into a database upon collection. One thing to note about the data collection is that you might need to schedule the collection, right? And you could use something like cron here and a lot of the different platforms out there provide some type of scheduled mechanism to wake up a process and then go collect data. And then you could shut down that process after you collect the data and put it into your database.
Play video starting at :1:57 and follow transcript1:57
Okay, so that's first off, we're collecting data for your project. The next bit is we're going to analyze the data. So the data analyzer is going to look into the database, get the data, and then do some type of analysis on that data. So this could be simply rolling up the data into a better reporting format, right, for your web application or you could go out and once again collect or, sorry, analyze data from a public API. So, if you were collecting Twitter data, for example, you might look for a sentiment analysis API or if you were collecting image data, you might do some image processing on that data. Okay, so that's the data analyzer and the last bit now is the web application. So, we have an end user, right, who's going to interact with your web application, and simply the web application is just going to respond with data, right? So that's going to be data that was analyzed from your data analyzer and again, that could be something rolled up to better improve a reporting view or truly going out and analyzing as I mentioned before. Okay, so those are the components for your system, there's one other thing to mention, which is a message queue and where you could use the message queue is a few different places. One, you could have the user trigger the message from the web application or signal to the data collector to go collect data. You could also take the same approach and signal to the analyzer to go analyze data. The message queue is pretty common and you'd find actually all of these, right, in a basic architecture that you might deploy to Amazon or Google Cloud. And I guess one last thing to mention is all of these components here, so this would be the larger system that you're going after. So, an application, right, a web application here, you have something that collects data here, you have something that analyzes data here. Naturally, you're putting data that you collected into the database, you're also then using the database to get data to analyze. And the message queue is simply to move things along a little faster than some type of polling interval that you might have with cron. Okay, so that's your whiteboard architecture, thanks.

0:04
We're here to talk about minimum viable products or MVPs. When you're building a minimum viable product, that means you're building the smallest thing possible that you can launch quickly with a goal of getting feedback from users, taking that feedback, adjusting, and then building again this graph here. It's a tweet by Mike, but it illustrates the idea of a minimum viable product really well. If you look at the 1998 side of the graph, this is the way software is built in cycles of 6-9 months. They should develop the software, then you test the software. Then finally you deploy the software and after that amount of time, you get feedback from users. The more modern way to build software is to invert that process. To deploy your software first, then write your automated tests and then develop. The key part here is you're doing that feedback cycle daily. Once you get the software out there and prod and user are using it, you take that feedback and put it right back into the cycle and improve. The idea here is that you haven't delivered any value until you do shift to prod. Let's shift to proud as quickly as we possibly can, we make sure we're delivering value for our users. Some of you might have seen this diagram before or a variation of it. But this for me really helps to drive home what an MVP is. Imagine that we're building a car. They're two different approaches that we can take to building a car. We could build the wheels and then the platform for the car, and then the body, and then finally the passenger compartment. That would be like that top row. But if we think about that, we haven't delivered any value along the way. What is a customer going to do with a wheel, or a platform, or a car without a passenger compartment. We haven't delivered any value for our users until step 4, and it turns out maybe the customers don't even like what we've built in step 4. A much better way to build software is to learn along the way. The idea is that we want to break down our problem into small increments and have the product at each one of those increments be useful in a way that we can get feedback from users. Take that feedback into consideration and build something better. If you imagine we're building a car for a person, what we really want to do is break that down and say, what's the problem that we're trying to solve for this person? The problem that we're trying to solve is transportation. The simplest thing that we could possibly could build might be a skateboard. It's not going to give the users a big wow moment. The users aren't going to be too excited about using a skateboard when they thought they were going to get a car. But they're going to give us feedback. Maybe they don't like the color of the skateboard. Maybe they want some other features. Maybe the wheels are squeaky, they'll give us feedback, then we can go and build a scooter, give that to more users. It's more along the lines that they were thinking they can give us some more feedback. We'll go back to drawing board and build the bike. Bike actually might solve most of the use cases that people need a car for. Some of our users might be fine with the bike. Maybe we're okay stopping there. But if not, we can iterate. Maybe build a motorcycle, get more feedback, and then go on and build our car, and if you notice here, the car that we ended up with in that second step when we're iterating, getting feedback as we go might not be the car that we had in mind at first. In this case, the car is a bit different, and that's the real power of focusing on an MVP is you get feedback along the way from your users and you might end up, or hopefully end up somewhere totally different than where you thought you might have. But somewhere better than where you thought you might have. You end up with a better product and good news. You've delivered value along the way. I encourage all of you to take this approach when you're looking at the projects for this course. Make sure before you do anything else, you get something working. Get something that you can deploy to production like environment, test out, maybe get some feedback, send it to some friends, and then iterate on that. Continually improve it and make sure that it's useful every step along the way. It'll be helpful in your projects and further on in your careers as you start to build more important software. Thanks and good luck.

In the last module, you identified the tech stack that you will be using for the project. Now you will get the chance to implement some of those technologies. First we will set up the code repository. Next we will discuss testing and continuous integration/deployment. We wrap up this lesson by integrating with a database.

Code Repository 
One of the first decisions a developer is faced with is: Where should my code be hosted? One of the major benefits of hosting code on a online repository includes the ability to implement version control as well as ease of collaboration. Popular code repository services include 
GitHub
, 
BitBucket
, and 
GitLab
.  Once you have decided on where you are going to host your repository, you will need to determine your branching strategy. While 
Gitflow
 is fairly popular, we tend to favor 
mainline
 development, where developers work on top of a branch called main and integrate their code with the shared repository multiple times per da

Continuous Integration/Deployment & Testing
Continuous Integration
 (CI) refers to automating the practice of frequently mergeing changes into the main branch of your codebase. Usually, for those changes to be merged, they have to undergo some sort of testing process. The testing process may consists of unit tests as well as integration tests. Hence to be able to continuously integrate new changes easily, an automatic mechanism should handle the testing process when new changes are introduced. Popular CI tools are 
Travis CI
 and 
GitHub Actions
. Generally CI tools outline a sequential set of steps that include building the project as well as running the appropriate tests. To tell the CI tool how the aforementioned process should be done, the developer creates a file that describes the process and adds it to the repository. Customarily, the file follows the YAML format. Continuous Deployment is the practise of ensuring that the latest changes to your product are reaching your production environment in a seamless fashion.

In the next module we discuss how you can use GitHub actions for Continuous Integration and 
Heroku Pipelines
 for Continuous Deployment. 






To get started on the development of your project, we will set up a simple website with a public facing website. We will use 
Flask
 as the web framework, GitHub to host the code repository, and Heroku to provide us with a public facing URL for the website.

Getting Started with Flask
To get started on development we shall create a virtual environment using Python and then upload the contents of the repository to GitHub.

To create a Python virtual environment in Linux, use the following commands:

Install Python's venv module:  sudo apt install python3-venv

Create a new directory for the application, and then make that directory your current working directory: mkdir my_flask_app && cd my_flask_app

Create the virtual environment and source the activation script:  python3 -m venv venv && source venv/bin/activate

Install Flask: pip install Flask

Once the virtual environment has been created, we can go ahead and create a simple website. To get started on the Flask application, follow the ensuing steps:

Create a source folder: mkdir src

 This is where majority of the source code for the project will reside.

Create the application file: touch src/app.py

Tell the system which file is the application file: export FLASK_APP=src/app.py

The next step is to add code to app.py. Insert the following code to the file:


#!/usr/bin/env python3

from flask import Flask, request

app = Flask(__name__)

@app.route("/")
def main():
    return '''
     <form action="/echo_user_input" method="POST">
         <input name="user_input">
         <input type="submit" value="Submit!">
     </form>
     '''

@app.route("/echo_user_input", methods=["POST"])
def echo_input():
    input_text = request.form.get("user_input", "")
    return "You entered: " + input_text


The code above creates a web page which contains a form that requests user input. Once the "Submit!" button is clicked, the user input is displayed as text on the web page. 

  5. To run the Flask application, go to the terminal and execute:  flask run. The application should be accessible     via this url: http://127.0.0.1:5000/. You should see a web page resembling the picture below.



Code Repository
Once we have verified that the application is able to run locally, we shall now upload the source code to GitHub. 

The first step is to create a repository on GitHub.com and get the URL for the remote repository. To do so, navigate to 
GitHub
, and click on the '+' icon on the top right corner of the screen. Then click "New repository".

You will be prompted to give your repository a name. Once you have given the repository a name, click the Project Structure"Create repository" button.

Afterwards, you will be presented with the URL for the remote repository. It typically follows the format git@github.com:<username>/<repo_name>.git.

Return to the root directory of your Flask application and initialize a Git reporistory as well as add and stage the files: git init && git add . && git commit -m "First Commit" && git branch -M main

Next add the URL for the remote origin: git remote add origin <remote_url>, using the remote URL from step 3.

Push to the remote repository: git push -u origin main

Now your code should be hosted on GitHub. If you need additional help, feel free to consult 
GitHub Docs
. 

Hosting on Heroku
Now that your code is on a remote repository, you are one step closer to deploying a web application. We will be using Heroku to deploy the Flask application. Heroku is one of many cloud platforms as a service (PaaS). Once the application is deployed, we will get a public URL that makes our application accessible by any device connected to the internet.  Follow the steps below to host your Flask application on Heroku:

To get things started, install Gunicorn: pip install gunicorn

We need to add two additional files to the remote repository.  The first file would be a text file that list the modules our application depends on. To do so execute: pip freeze > requirements.txt

Next we need to create a Procfile at the root of the directory. A Procfile tells Heroku how to run our application. We create the Procfile using: echo "web: gunicorn src.app:app" > Procfile

Once you have created the two aforementioned files, push them to your remote repository.

If you do not have an account with Heroku, you need to 
create an account
. 

Once you have logged in and accessed the dashboard, click on the "New" button located on the top right corner of the screen. Afterwards, select "Create new app".



   7.  After creating the app, you should be redirected to a screen resembling the image below. The next step entails connecting your GitHub repository to the Heroku application. Click on the "GitHub Connect to GitHub" button.

Case Study
In this portion of the course, please review 
this paper
 (
alternate link
) on Agile development used in the real world. The case study titled "Agile Requirements Engineering and Software Planning for a Digital Health Platform to Engage the Effects of Isolation Caused by Social Distancing1" discusses the development of a mobile application called ADAPT-CAFÉ. This app is designed to mitigate the effects of social isolation due to social distancing measures, particularly among older citizens. The app uses the behavior change wheel (BCW) framework to guide the development of behavior change interventions. It also uses the behavior change techniques (BCT) taxonomy to provide a clear description of the specific techniques being used in the app.

The anonymized data from the app will be used to understand people's movement within lockdown regions and the occurrence of symptoms within the user group. This data will be used to create dynamic stratified patient demand forecasts with both machine learning transferred parameters and an agent-based simulation of patient demographics. The data will also be used to develop a geographical model superimposed on a health system's capacity to serve care, which will be analyzed against current and future availability of care delivery resources.

The software product development follows the agile framework defined by the UK Government Service Manual, as well as lean methodology and iterative design and development sprints. The approach relies on reuse and iterative improvements of the code and user experience elements, in particular via deploying open-source tools and codebase, conducting fast validation with real users, and maintaining consistent performance measurements against predefined key performance indicators, including adoption and retention metrics.

The evaluation of the app's feasibility, acceptability, and usability will be conducted using several scales and theoretical models and frameworks. A feasibility study will be initiated and last 12 months; this will include a 1-month evaluation and intervention refinement, and an 11-month implementation and follow-up. The study is centered on the app and excludes the reporting engine; the basis for this is that successful uptake of the app is required for downstream data in the reporting engine to be useful.

The technology development will follow a modular approach following the representational state transfer application program interface structure. Backend app databases will be hosted on the Amazon Web Services (AWS) infrastructure, complying with General Data Protection Regulation-compliant levels of data security and enabling big data processing. At the core of the system, implementation is an artificial intelligence (AI)-powered chatbot agent, deployed with an open-source framework such as Amazon Lex. The integrated machine learning modules will give users a personalized experience to improve the quality of question prediction and response.





1Meinert, Edward, et al. "Agile requirements engineering and software  planning for a digital health platform to engage the effects of  isolation caused by social distancing: case study." JMIR public health and surveillance 6.2 (2020): e19297.

0:04
Welcome back. Let's talk about testing for your project. So there's a few types of tests that you might hear about. So maybe integration you've heard, maybe acceptance, maybe even unit tests. And I guess I'd like to start out by saying let's just focus on writing some fast tests. And those tests are intended for developers that we could run within our integrated development environment, or we could also run within a continuous integration or delivery environment. So I have a few tests here and this is Provenance, this is a small application that should look a lot like what you're putting together for your project. This one in this case is, I think it's a blend of Kotlin and Java, but let's dive in here for a second. And if I look at a test, so naturally I'd annotate and somehow talk about the fact that I'm testing. We could also look at the directory structure and with a Java or a Kotlin project, or I guess more specifically with a Gradle structure, we'll have a couple of directories. So within components articles, I have source main and then I have source test. And within source test is where we're going to have our test cases. So the first one, we could see what I'm doing here. So basically I create a new gateway, and this is where we're going to store articles that we're looking at. And I have a simple test, right, that is Find All. And then I assert that I got the correct size back, and then I could find available, and I assert that I get the available size back. And where that's coming in is here. So this is where I'm annotating or marking the record as available. And so we could see that a handful are available, four are available out of the six total. So if I right click and run this test, we get back some data and we assert that we've actually found all of our records and then we've also found all of our available records. And while this test isn't necessarily testing a database directly, I have just this articles data gateway. And at the top of it, it has a list or an array list of articles. Traditionally a data gateway is going to be talking to your database. So think about replacing some of the code here with SQL statements. And then when you're running your gateway test, you would actually be running the test against a SQL database like Postgres. So let me show you one other test. So again, this is a web application, or in this case we might even be exposing an API, and we have controller tests as well. And so in this case we're doing a little bit of the same with our data gateway. So we're storing some information within our database. The difference here is we're actually starting our basic application or our app server. And then in the test for Find All, we're actually making a real rest request. So if I run this one, and I call localhost, and this is the port that I'm running on, I get back all of the articles. Okay, so these are a few examples of testing and what we might be looking for or what we will be looking for in your project. Thanks.


Unit Tests
Unit testing is the practise of testing individual program components such as object classes or methods. Unit testing is part of the routine for product development. Consequently, whilst you are developing a code unit, you should be developing tests in tandem. For extensive projects and safety critical software, a dedicated team may handle testing.

To illustrate the process of unit testing, we shall first define a Calculator class in Python that contains two methods, namely, add and subtract. Next we will write unit tests for the aformentioned methods.

#!/usr/bin/env python3

'''
A simple calculator class in Python
'''

class Calculator:
    def add(self, a, b):
        return a + b
    
    def subtract(self, a, b):
        return a - b

Once we have the class defined, we now create a seperate Python file to handle the unit tests.



#!/usr/bin/env python3
'''
Import unittest framework and the Calculator class
we defined.
'''
import unittest
from Calculator import Calculator

'''
Test cases are created by subclassing unittest.TestCase
'''
class TestCalculator(unittest.TestCase):
    '''
    setUp function is used to instantiate the object we are testing.
    '''
    def setUp(self):
        self.calculator = Calculator()

    '''
    Test the add function.
    '''
    def test_add(self):
        add_result = self.calculator.add(1, 2)
        self.assertEqual(add_result, 3)

    '''
    Test the subtract function.
    '''
    def test_subtract(self):
        subtract_result = self.calculator.subtract(1, 1)
        self.assertEqual(subtract_result, 0)


if __name__ == '__main__':
    unittest.main()


As shown above, a unit test is written for each method and the test class is meant to test a single object. For each method we compare the result returned from the method against the expected result.  If the two are equal, then the test is passing.Therefore, the example above illustrates the three main components of a unit test: setup, invoking the method and assertion.  For some of the methods you use in your project, you are expected to write unit tests for them. For detailed usage of the Python's unit testing framework, check out the 
docs
.


Mock Objects
Mock objects belong to the category of test doubles. They are used to create a controlled instance of an object within a testing environment. Sometimes, it is preferred to use a mock object rather that an actual instance since we can isolate and simulate the behavior of certain features. For example, let us say our Calculator has a currency conversion feature, and we would use an API to fetch the conversion rate. However, conversion rates usually fluctuate and as a result of this we can't consistently predict the expected conversion. To remedy this, we create a mock object that always returns a consistent exchange rate so that our test can be executed.

To demonstrate the use of mock objects in testing, we alter our calculator class to incorporate the currency conversion feature. For now, our calculator can only convert United States Dollars (USD) to British Pound (GBP). To assist with the conversion, we create a helper method called _get_exchange_ratewhich makes an API call to get the latest currency exchange ratio. 



#!/usr/bin/env python3

import requests


class Calculator:
    def add(self, a, b):
        return a + b

    def subtract(self, a, b):
        return a - b
    
    '''
    Get the value of 1 USD to GBP
    '''
    def _get_exchange_rate(self):
        response = requests.get("https://api.frankfurter.app/latest?from=USD&to=GBP")
        return response.json()['rates']['GBP']
    
    '''
    Do the conversion
    '''
    def usd_to_gbp(self, amt_in_usd):
        exchange_rate = self._get_exchange_rate()
        return amt_in_usd * exchange_rate

Currency rates fluctaute frequently. As a result of this, we would not currently be able to assert an expected value to test the the usd_to_gbp function defined in line 23. One day the ratio may be 0.81 and another day it could be 0.82. Therefore, to control the test, we need to simulate a consistent return value for _get_exchange_rated. 

#!/usr/bin/env python3
'''
Import unittest framework and the Calculator class
we defined.
'''
import unittest
from Calculator import Calculator
from unittest.mock import MagicMock


'''
Test cases are created by subclassing unittest.TestCase
'''
class TestCalculator(unittest.TestCase):
    '''
    setUp function is used to instantiate the object we are testing.
    '''
    def setUp(self):
        self.calculator = Calculator()
        self.calculator._get_exchange_rate = MagicMock(return_value=0.82)

    '''
    Test the add function.
    '''
    def test_add(self):
        add_result = self.calculator.add(1, 2)
        self.assertEqual(add_result, 3)

    '''
    Test the subtract function
    '''
    def test_subtract(self):
        subtract_result = self.calculator.subtract(1, 1)
        self.assertEqual(subtract_result, 0)

    '''
    Test currency conversion
    '''
    def test_currency_converter(self):
        conversion_result = self.calculator.usd_to_gbp(10)
        self.assertEqual(conversion_result, 8.2)


if __name__ == '__main__':
    unittest.main()

As can be seen in line 20, we use the MagicMock module to be sure that the same conversion rate would always be returned from the function which sets the exchange rate. This is useful, because as can be seen in line 41, we have an expected value which we can use to compare the result of the functiuon call.

Now that you have learnt some of the basics relating to unit testing, you can use this foundation to develop unit tests for your course project.

Integration Tests
Integration testing is the practise of testing the interoperability between different software modules. In other words, integration testing combines differenet software modules and test them as a  logical group. To demonstrate integration testing, imagine with have a server process and a client process. The client submits a request to add two numbers and the server responds with the resultant sum. To test the two components, we instantiate both the server and client and then submit a request from the client to the server. We then check the response from the server to ensure that it is what we expected. We can illustrate this using an example.

We define the server as follows:

#!/usr/bin/env python3

import socket

'''
Server listens for request containing two numbers
and then sums those numbers. The server responds with
the summation
'''

class Server:

    def __init__(self, host=socket.gethostbyname("localhost"), port=54003):
        self.socket = socket.socket()
        self.socket.bind((host, port))

    def close(self):
        self.socket.close()

    def _add(self, a, b):
        return str(int(a) + int(b))

    def listen(self):
        self.socket.listen(1)
        conn, addr = self.socket.accept()
        while True:
            data = conn.recv(1024).decode()
            numbers = data.split(" ")
            sum = self._add(numbers[0], numbers[1])
            conn.send(sum.encode())

if __name__ == "__main__":
    server = Server()
    server.listen()

The source code for the client is shown below.

#!/usr/bin/env python3

import socket

class Client:
    def __init__(self, host=socket.gethostbyname('localhost'), port=54003):
        self.socket = socket.socket()
        self.socket.connect((host, port))

    def close(self):
        self.socket.close()

    '''
    Send two numbers to be added to Server
    Numbers are sent as strings and response 
    is the summation.
    '''
    def get_sum(self, a, b):
        message = str(a) + " " + str(b)
        self.socket.send(message.encode())

        response, _ = self.socket.recv(1024).decode()
        return response

We can therefore make a simple to test which invokes both the client and server.

#!/usr/bin/env python3

'''
This file is called test.py
'''
from subprocess import Popen
from Client import Client

def test_server_client():
    '''
    We start the server and let it run in the background. Then we ask 
    the client to make a call to the server and we compare the expected value.
    '''
    server = Popen('./Server.py')
    client = Client()
    sum = client.get_sum(3, 4)
    assert (sum == 7)

We run using pytest test.py .

In your project, you will be required to design and implement some integration tests. Some worthwhile tests could be an integration test that involves the a data collector and a data analyzer or a messaging queue and database. 

0:03
We are going to talk about continuous integration and continuous delivery. We're back to our Kotlin-Ktor-starter application. If we look into our application's directory, we'll find that there are a few tests in here. Not very many, but we included a few for demonstration purposes, particularly this demonstration. We have our app test. If you look in here, we're starting up our application, in this case, a web application. Then we're making a call to slash basically to ensure that we're able to see the homepage. We get a 200 response. Then we also assert that the page contains an example application using Kotlin and Ktor. When you're putting together your application, you're going to have a handful of tests and at the command line or within your IDE, you're naturally going to be running those tests to ensure that your application works properly. When we think about continuous integration, what we want to do is run the test suite continuously, or we want to run the test suite on every commit to GitHub. To do that, we're going to use GitHub actions. If I bounce back to the top directory and go into GitHub workflows. This is the manner in which you specify your GitHub action. In here, there's a build.yml file, and you'll notice that we have a few things. At the top, we have how frequent we're going to build. This is just simply on push. Alternatives here might be building on a specific branch, but we're just going to build or test, in this case, our application on every push or every commit. So we have a few jobs. This first job is what embodies the continuous integration aspect of our project. Every time we push, we're going to run gradle build. Gradle build before it actually builds the Java archive file or the artifact for us, it's going to run our test suite. This ensures that all of our tests get run on every push. That is fundamental and this is basically what we're getting at when we say continuous integration. So run all of our tests on push or on commit to GitHub. The next bit. When we switch to continuous delivery, that's now pushing our application to a review or a production environment. In order to do that, we need to do a few things. One, we need to pack our container or package docker container so that we could give it to in this case, Google is our target platform that we're deploying to. Google comes with a few GitHub actions, and it's mainly around calling G-Cloud commands. For this Kotlin-Ktor-starter project, we're using Cloud-build and then Cloud-run. Cloud-build is what's going to build our container for us and then put it into Google's container registry. A container registry is just simply a place where different platforms, whether it's Google, Amazon, or Azure keep those Docker containers and make them available for you so that you can deploy your app to a review or a production environment. The first step in continuous delivery for us is using cloud-build to build our Docker container and publish it to our registry. That's Step 1 for continuous delivery. Step 2, is that we then need to deploy the application. In this case, we're also going to use the GCloud command. This is our basic web server, our basic app that we're deploying. Again, we need to authenticate with credentials. I guess I didn't mention, but these credentials are part of GitHub and the credentials are secrets that are made available to GitHub actions when you're running them. You'll see in settings that I'm setting Google credentials there. Those secrets then are available for GitHub actions to use. In this case, it's what's letting us log in to Google Cloud. This command then is going to implement the Cloud Run command, which is going to lead us deploy an application or redeploy an application. We're doing that actually for all three of our applications. Remember, a rubric we talked about. We have three different processes or three different applications that's part of our architecture. This is the first, the basic server, our web application. We have the data analyzer and then we have the data collector and all three are going to be deployed, in this case to Google Cloud using the Cloud Run commands. Maybe one thing that's important, it's worth mentioning that I didn't share the actual Cloud Run commands here or the Cloud build command up top. There's a variety of different ways of deploying and there's a ton of literature, whether it's on Google or whether it's on one of the more popular blogging tools that are out there. I'll leave it up to you in order to deploy. If you're going to deploy to Google as your target platform, I would say the same holds too for Amazon and Azure. Then even something like Heroku, if that was your target platform for your review and production environment, you could use the same commands here. To summarize, we're using GitHub actions for our continuous integration, which runs our test suite. Then we're using also GitHub actions for our continuous delivery. In this case, there's two steps to continuous delivery. One, building the container, and then two, deploying the application. Thanks all, good luck with continuous integration and continuous delivery.

0:03
Take a look at data persistence and saving data to the database. For this example, we're going to look at an exercise from our first course, and that was the milk problem. If I take a look at this application, in our milk problem was a component and within there we had products. There's maybe an interesting thing here where we have a pairing of records and then data gateways. A data gateway is a pattern or design pattern, and we're using that pattern to store off records and, in this case, a database. Let's look at our record though first. A record is pretty straightforward. This is just a data class. We call it product, is what we're getting at here in terms of our milk problem. Then we tack on record to that. The information here, naturally we're going to have an ID, something to identify the record, and we have a name and a quantity. Here's what we're doing with our milk problem in terms of records. If we look at the data gateway, there's a couple of things going on, but typically you'll see something along the lines of create, read, update, delete. Read is typically list and show. The combination of those is typically referred to as crud, so C-R-U-D. That's simply getting data, and a lot of times it's configuration data or information that you're displaying to the user in the database, so in and out of a database. Naturally we have a function here called create. We're using a database templates and then we give that our data source. There's a number of ways to get data in and out of a database. The template method or this template pattern is fairly common. It gives us a nice way to then insert data and then immediately map that data back to our product record, and the name and the quantity is what's going into the table. That template pattern gives us a really nice way to do mapping. You might have heard of things like object relational mapping tools. This is really just the mapping side of things, not necessarily a full ORM. We have a create, we have findBy. In this case, we're finding by ID, and then we have update. Then in our milk problem, we were dealing with gallons of milk and we were incrementing and decrementing the quantity of milk. But for your project, you might have something else unrelated to milk or incrementing and decrementing, but you may have different functions or methods in terms of getting information from the database. This is the milk problem, and this is what data persistence looks like. In here, we have a test as well. If we look at this test, typically what we're doing is getting some data in and our before method or function. We're deleting from products and then we insert a few products into a database. As a reminder from testing, the before happens before each test. Before our create, we'll have some data in the database. Before select, naturally we'll have some data and then our findBy and update as well. The important thing here is that we're using our data gateway, our product data gateway for doing our find, and then we're asserting against the actual data that we've gotten. We're not mocking anything here. It's important to test your database with unit tests or at this level. Let's call him developer tests for the moment or tests that are just simply fast. Don't mock. This is the only place that you're actually going to test the sequel that you're writing, so write tests. Otherwise, this will go untested within your codebase, this insert statement. Well, that's what data persistence looks like. This is very much tied to collecting data. Then the next video, we're going to talk about collecting data and then persisting it. What we're going to be looking at is collecting data from an endpoint. In this case, we're going to get data from your InfoQ and then we're going to store that data in our endpoint data gateway. In this case, we didn't plug in a real database. We're actually just using an empty collection for the moment. But more to come on the next video. Thanks. That's the milk problem and that is persisting data with your product data gateway. Thanks.


Data Persistence
For your project, you will be handling data. Conequently, you may need your data to be stored somewhere so that you can use said data later on. A database is one of the best ways to store data for a web application. When choosing a database a few key factors should be considered:

SQL/NoSQL

Ease of implementation

Security

Scalability

Cost/Sustainability 

Perhaps one of the biggest factors influencing your choice would be wether you want a SQL database or a NoSQL database. Some key comparison include:

Since NoSQL have a dynamic schema, they are better for unstructured data. SQL by definition has a well defined schema and thus is better for table based data.

NoSQL is horizontally scalable whereas SQL is vertically scalable. More info can be found 
here
.

For your project, you have discression in choosing whichever database software you think is best. However for the novice, we show how to use SQLAlchemy with Flask to store user information. SQLAlchemy, as the name suggests, is a SQL database and is great for storing user data (name, email etc) since they are relational entities.

First we install the package using pip.

pip install flask-sqlalchemy

Once the package is intalled, we need to create a Flask application object, set the database URI and then initialize a model. Below we create a database model called Users which is what we can use to store information about each user. Line 10 is the primary key for the database and it is a numberic idetifier. Line 11 is the name of the user. We can create additional attributes if we wanted.

.


#!/usr/bin/env python3
from flask import Flask
from flask_sqlalchemy import SQLAlchemy

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///Users.sqlite3'

db = SQLAlchemy(app)
class Users(db.Model):
    id = db.Column("User_ID", db.integer, primary_key=True)
    name = db.Column(db.String(20))
    

Data Collection
Your project may require you to fetch data from an outside source. REST APIs provide a great means for retreving data. To demonstrate data collection, we shall create a simple Python script that fetches data from an API and stores it in a database. For our example, we query the current temperature from 
weatherDB
 using their API. Once we get the data, we store it in a database.

#!/usr/bin/env python3
import requests
from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from datetime import datetime

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///Weather.sqlite3'

'''
Define the database model
that is used to store 
the temperature.
'''

db = SQLAlchemy(app)
class Weather(db.Model):
    datetime = db.Column(db.DateTime, primary_key=True, default=datetime.utcnow())
    temperature = db.Column(db.Integer, nullable=False)
    
'''
Helper function to get temperature
using API
'''

def get_temperature():
    response = requests.get("https://weatherdbi.herokuapp.com/data/weather/boulder")
    return response.json()["currentConditions"]["temp"]["c"]


'''
In main we first get the current temperature and then 
create a new object that we can add to the database. 
'''
if __name__ == "__main__":
    current_temperature = get_temperature()
    new_entry = Weather(temperature=current_temperature)
    db.session.add(new_entry)
    db.session.commit()


0:04
Let's take a look at our API request that we need as part of our project. We're going to go back into our provenance application that we saw with our first course in our three-part series, and so if we go into components directory endpoints, within here we'll see source main Java, then our endpoints package. There's a worker in here, and it's this worker that's reaching out to an endpoint to collect data. The one thing maybe to call out here is we do have a workflow support directory. This is in some ways it's imitating or acting like fun. For your project, you might be able to use cron or some type of scheduled task. In this case, we wrote our own workflow support to do that scheduling. If we look at the worker, which is the class, the method that ultimately gets called to go out and collect data from the endpoint. I can open up this file. In here, we're going to have an execute method. This is like a runnable task and Java. In here, what we're doing is we're using a rest template. This template goes out and makes a request. If we look at this rest template for the moment, there's a few things in here, and this is much like curl. It's got to get command, it's got a Post command. Then if we bounce back to our Execute method within our endpoint worker, it we'll see that we're executing that get command, and then in here there's a couple of things that get triggered that are required. One is the endpoint itself. Then the other bit is the info about the task, if we looked at this getAccept, this is accepting applications at XML. In this case, the reason for that is because we're actually getting RSS data from an endpoint. In this case, I believe we're getting it from infoQ is how this project was set up, and so we get back a response. Then once we have that response, we map it to or we should say Marshall, the XML. We get some RSS data. Ultimately what we're doing is we're going through a list of RSS items. Then we're saving those in our database and we're using our articles Data Gateway as the class or the method save to get those into our database. That's what we're looking for when we talk about collecting data from endpoint. In this case, just to play back, we're using a rest template, which has a get request. This is much like using curl from the command line. We get back some XML, we tick that XML and we map it into a class or an object structure and then from that, we go through our XML data and more saving off the titles that we found on this line here. That's what we're getting at when we talked about collecting data from a rest endpoint, the tests for that might be worth showing as well. If I go into here, I have my endpoint worker and then I have my endpoint worker test. You could see here that we're actually talking about getting info from infoQ. This is mocked, we're actually not getting intentional. I'm not putting the URL in here to ensure that I'm mocking. Ultimately we go back. Can we find that we've collected 15 records from infoQ when we execute our worker. That's both endpoint worker and endpoint worker test. In this case, we're going out and collecting RSS data from infoQ. Thank you.Get


Data Analysis & REST API
APIs are great for fetching raw data, however, sometimes they may not offer an endpoint that provides analyzed data. Furthermore, since you now have a database that stores information, it would be great to do some analysis on that data. To demonstrate data analysis, we showcase a snippet of a Flask API endpoint which returns the average temperature from a range of dates. 


0:04
We are back with our provenance application, and what we're going to talk about now is metrics and monitoring. So there's a few things to call out with metrics and monitoring. So I have a new package here, and it's called metrics support. There's two classes in here, one is a health check and the other is a metrics controller. And so let's talk about the health check first. So this is fairly straightforward, and really all I'm doing here is exposing a get request to health check, and I'm rendering a response. In this case, I'm rendering a text response saying that, hey, I'm healthy. And so this is important that there's an endpoint on your application that simply just tells someone of it who's interested whether your application is healthy or not. And if you think of when you're deploying your application to a platform, you might even be using something like Kubernetes.
Play video starting at :1:9 and follow transcript1:09
All of those platforms tend to require a health check. And it's how the platform knows that your application is healthy and such that it could respond and potentially email or text you and say, hey, your application is down. So that's the first bit is the health check where that gets wired in. So this is our application up top, and we're starting our application. And one of the endpoints that we talk about here is our health check. So this is registering controllers or registering endpoints with our application. Okay, so that's the health check. We even have a little test for the health check that we could look here. And again, it's saying I'm healthy. So we'll also be per the last video looking to see whether or not you're testing your endpoints for metrics and monitoring. Okay, so that's monitoring, let's switch over to metrics. And in the test above, we have a test for metrics. And what we're getting back is data on the number of requests that we might be making to one of our endpoints. So if I bounce back into articles for a minute and I look at our articles controller, you'll notice something, right, that in here, I'm actually giving the articles controller some information about requests, right? So I pass in this metrics registry, right, and then I actually register an article's request and then an article's available request. And these are just meters, right? So these are just looking at the number of requests to either of these endpoints and then displaying that meter to the user so you could see it during one of these, we basically say we put a mark in here, right? And this is signaling to the metrics registry that somebody went through this method and we responded accordingly. Okay, so where metrics get wired in is a similar place in the application, right, it's just the line below, so metrics controller. And this is where we're baking in also Prometheus. So Prometheus is a place or application that collects data, and then Grafana is typically used to display that data in a graphical user interface. Prometheus, you could think of as a database much like postgres, although it's specialized for collecting metrics data. So when we start up our application, here's our main method for our Java class. This is going in and starting our application. This little basic app that we created here, it has a couple of things, right? The app that we have extends basic app, and in order to register those handlers, we need to do just that. And these are the lines of code that I was talking about to register both our health check and our metrics controller. And then, naturally, just above is where we register our articles controller. Okay, so this is what some code looks like as you're thinking about baking in metrics and monitoring to your application. And naturally, we're going to send that data to something like Prometheus, and then also visualize that data with something like Grafana. Okay, thank you.


Production Monitoring
Read the 
Monitoring chapter
 of Google's Site Reliability Engineering Workbook for an overview of a modern monitoring strategy.

Next, take a look at the 
Alerting on SLOs
 chapter for a perspective on alerting using tools like 
Prometheus
.



Event Collaboration Messaging
As you would have realized by now, your project comprises of different components. One of the key challenges with compartmentalizing software, is the ability of the components to communicate with each other. You may be tempted to use a Pipe, Socket or some other POSIX tool for communication, however they may not provide the best solution. This is where a messaging queueing software comes in. A message queue is an asynchronous data structure used for communication. Messages are stored on the queue until they are consumed. A producer is the term used for the process that populates the queue. In addition to a producer and consumer, a messaging queue also consits of a message broker. The broker acts as a middleman which receives messages from the producer and then populates the queues of the consumers. For a consumer to be able to receive messages, it must first subscribe to the messaging queue via the broker.

Before continuing, read Martin Fowler's 
excellent overview of event collaboration
.

The contents of the messages are flexible, they can consists of documents, commands or even events. To demonstrate event collaboration messaging, we shall create a messaging queue which handles credit card transactions. There are a number of clients for RabbitMQ. Since we are using Python, we will be using 
Pika
. Therefore, if you do not have Pika nor RabbitMQ installed, you will need to install both.

Visit the 
RabbitMQ installation site
 for instructions on installing RabbitMQ.

To install  Pika, use the Python package management tool: python -m pip install pika --upgrade

Below is an example of a producer. The producer is mimicing a credit card transaction and sends the transaction amount and credit card information to the queue. 

#!/usr/bin/env python3
import json, pika

'''
Establish a connection to a RabbitMQ server.
localhost means we are connecting to the local
machine. However we can provide a IP address to
a different machine.
'''
conn = pika.BlockingConnection(pika.ConnectionParameters("localhost"))
channel = conn.channel()

'''
We create a queue just for transactions
'''
channel.queue_declare(queue="transactions")

channel.basic_publish(exchange="", routing_key="transactions",
                      body=json.dumps({
                          "card_num": 12340000,
                          "total": 4.10
                      }))


Subsequently, a consumer (the credit card server), retreives the data and then concludes the transaction. As such the source code for the consumer would resemble the code block shown below.

#!/usr/bin/env python3
import pika, sys, os, json

def main():
    connection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))
    channel = connection.channel()

    channel.queue_declare(queue='transactions')

    def callback(ch, method, properties, body):
        body = json.loads(body)
        print(" [x] Received %r" % body)

    channel.basic_consume(queue='transactions', on_message_callback=callback, auto_ack=True)

    print(' [*] Waiting for messages. To exit press CTRL+C')
    channel.start_consuming()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('Interrupted')
        try:
            sys.exit(0)
        except SystemExit:
            os._exit(0)



